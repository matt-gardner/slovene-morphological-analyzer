\documentclass[11pt,letterpaper]{article}
\usepackage{fullpage}
\usepackage[pdftex]{graphicx}
\usepackage{amsfonts,eucal,amsbsy,amsopn,amsmath}
\usepackage{url}
\usepackage[sort&compress]{natbib}
\usepackage{natbibspacing}
\usepackage{latexsym}
\usepackage{wasysym} 
\usepackage{rotating}
\usepackage{fancyhdr}
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}
\usepackage[dvipsnames,usenames]{color}
\usepackage{multicol}
\definecolor{orange}{rgb}{1,0.5,0}
\usepackage{caption}
\renewcommand{\captionfont}{\small}
\setlength{\oddsidemargin}{-0.04cm}
\setlength{\textwidth}{16.59cm}
\setlength{\topmargin}{-0.04cm}
\setlength{\headheight}{0in}
\setlength{\headsep}{0in}
\setlength{\textheight}{22.94cm}
\newcommand{\ignore}[1]{}
\newenvironment{enumeratesquish}{\begin{list}{\addtocounter{enumi}{1}\arabic{enumi}.}{\setlength{\itemsep}{-0.25em}\setlength{\leftmargin}{1em}\addtolength{\leftmargin}{\labelsep}}}{\end{list}}
\newenvironment{itemizesquish}{\begin{list}{\setcounter{enumi}{0}\labelitemi}{\setlength{\itemsep}{-0.25em}\setlength{\labelwidth}{0.5em}\setlength{\leftmargin}{\labelwidth}\addtolength{\leftmargin}{\labelsep}}}{\end{list}}

\bibpunct{(}{)}{;}{a}{,}{,}
\newcommand{\nascomment}[1]{\textcolor{blue}{\textbf{[#1 --NAS]}}}


\pagestyle{fancy}
\lhead{}
\chead{}
\rhead{}
\lfoot{}
\cfoot{\thepage~of \pageref{lastpage}}
\rfoot{}
\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}


\title{11-712:  NLP Lab Report\\Slovene}
\author{Matt Gardner}
\date{April 26, 2013}

\begin{document}
\maketitle
\begin{abstract}
\nascomment{one paragraph here summarizing what the paper is about}
\end{abstract}

\nascomment{brief introduction}

\section{Basic Information about Slovene}

Slovene is a Slavic language spoken in the country of Slovenia.  It is spoken
natively by around 2 million people, most of whom live in Slovenia.  Slovene is
one of the official languages of the European Union, so there is a moderate
amount of data available for computational linguists.  Slovene's closest
relative is Serbo-Croatian, being mutually intelligible with some variants of
Croatian.  As with many Slavic languages, Slovene is highly inflected and has
relatively free word order~\citep{wikipedia-slovene}.

\section{Past Work on the Morphology of Slovene}

There has been some work on the Slovene language by linguists.  There is a
grammar written by \cite{greenberg-2006-slovene-grammar} that gives some 75
pages to the morphology of the language.  There is also a more comprehensive
grammar written by \cite{herrity-2000-slovene-grammar}.  There are a few other
papers on Slovene morphology to be found in English by searching Google scholar
(e.g., \cite{bidwell-1969-outline-slovene-morphology}), and more that is
written in Slovene.

As far as work in computational linguists on Slovene, a small body of
literature exists.  Toma\v{z} Erjavec, from the Jo\v{z}ef Stefan Institute,
published a description of an early morphological analyzer for Slovene.  The
paper is only a two-and-a-half page, very high-level description, and the
system was implemented in Prolog on a VAX
machine~\citep{erjavec-1990-slovene-analyzer}.  Erjavec also was a main
contributor to the Slovene Dependency Treebank, which includes morphological
information and was a part of the CoNLL-X Shared Task on multilingual
dependency parsing in 2006~\citep{dvzeroski-2006-sdt}.

More recently, there was a Slovene morphological analyzer released on
SourceFourge in May of 2012.  This analyzer uses a maximum entropy model and
achieves 92.5\% accuracy~\citep{grcar-2012-obeliks}.  The system (called
Obeliks) was trained on a corpus of five hundred thousand words.

\section{Available Resources}

The available resources for work on the Slovene language consist largely of
things mentioned in the previous section.  I already listed reference grammars,
the Slovene dependency treebank, and a corpus available with the Obeliks tool.
In addition, there is a Slovene lexicon available with the MULTEXT-East
dataset~\citep{erjavec-1998-slovene-lexicon}; I have requested access to the
lexicon and hope to have it soon.  Assuming I get access to the lexicon, I will
use the lexicon as I develop the analyzer, and I will use the morphology labels
from the Slovene Dependency Treebank and from the dataset released with the
Obeliks system as my two test sets.  These two datasets have hundreds of
thousands of tokens (though I have not yet counted the number of word types),
and the lexicon has over 15,000 lemmas, with full inflectional paradigms for
each lemma.

\section{Survey of Phenomena in Slovene}

For the phenomena my morphological analyzer will cover, I will follow exactly
the morphosyntactic specifications given by~\cite{erjavec-mds}.  This
specification gives twelve parts of speech.  Each part of speech has a set of
features or attributes that are marked in words of that part of speech.  These
are the parts of speech, with their features:

\begin{itemize}
  \item \textbf{Noun}: type (common / proper), gender
    (masculine / feminine / neuter), number (singular / dual / plural),
    case (nominative / genitive / dative / accusative / locative /
    instrumental), animate (yes / no)
  \item \textbf{Verb}: type (main / auxiliary), aspect (perfective /
    progressive / biaspectual), form (infinitive / supine / participle /
    present / future / conditional / imperative), person (first / second /
    third), number, gender, negative (yes / no)
  \item \textbf{Preposition}: case (specifies the case it governs, not the case
    it is inflected for)
  \item \textbf{Conjunction}: type (coordinating / subordinating)
  \item \textbf{Adjective}: type (general / possessive / participle), degree
    (positive / comparative / superlative), gender, number, case, definiteness
    (yes / no)
  \item \textbf{Pronoun}: type (personal / possessive / demonstrative /
    relative / reflexive / general / interrogative / indefinite / negative),
    person, gender, number, case, owner number (for possessive pronouns), owner
    gender, clitic (yes / bound)
  \item \textbf{Adverb}: type (general / participle), degree
  \item \textbf{Numeral}: form (digit / roman / letter), type (cardinal /
    ordinal / pronominal / special), gedner, number, case, definiteness
  \item \textbf{Particle}: (no features)
  \item \textbf{Interjection}: (no features)
  \item \textbf{Abbreviation}: (no features)
  \item \textbf{Residual}: type (foreign / typo / program)
\end{itemize}

This list is roughly in the order I would assign them priority.  I doubt that I
would be able to handle open class abbreviations or interjections, or the
loosely-defined ``residual'' part of speech.  The closed classes should be
relatively easy, as I have a large corpus that has been labeled with these
morphosyntactic descriptions, so I will only need to produce a few small
lexicons for these words.  I will probably not try to handle common nouns vs.
proper nouns at first, nor to distinguish main verb vs. auxiliary verb (which
depends on context).  Aspect of verbs will also be difficult, and so I will
save it for later, and there are a few features listed above that only rarely
show up in word forms (like noun animacy and verb negativity) which could be
ignored at the beginning.

\section{Initial Design}

The morphological analyzer is designed as follows.  I am using as my working
data set a large lexicon called Sloleks generously provided by Toma\v{z}
Erjavec (different from the MULTEXT-East lexicon mentioned above).  This
lexicon has about 930,000 inflected word forms together with their lemma and
morphosyntactic disambiguation, so it is a perfect test set for my
morphological analyzer.  I processed the lexicon and split it into 12
individula lexicons, one for each main part of speech listed in the previous
section, using the lemma provided as the lexical entry.  I then created 12
individual test sets, again one for each part of speech, grouping all analyses
of an individual form.  I wrote automated tools to test each part of speech
independently, using the lexicon and test set for that part of speech, together
with the inflection rules written for each part of speech in separate files.
The organization of the system, then, is the following:

\begin{itemize}
  \item The base lexicon and automatic processing files:
    \begin{itemize}
      \item The base lexicon, with approximately 3 million possible analyses of
        approximately 1 million word forms.
      \item A python script that processes the base lexicon and produces 12
        individual lexicons, one for each part of speech.  This script does
        some simple analysis of the lemmas to determine proper continuation
        lexicons for each word, when that is necessary (e.g., adjectives that
        end in -ev, -o\v{c}/-e\v{c}, and -i each have their own contiuation
        classes, because they have different analyses).
      \item Anther python script that processes the base lexicon and produces
        12 test files, one for each part of speech, and a single test file for
        all parts of speech at the same time (this separation is helpful, for
        instance, when a single word could have more than one part of speech; I
        can see how my analyzer works on adjectives without having confounding
        results from forms that could also be nouns).
    \end{itemize}
  \item The hand-written lexicon files:
    \begin{itemize}
      \item A base file that gives all of the possible parts of speech, and
        multicharacter symbols.
      \item For each part of speech, a rules file that adds endings and the
        inflectional meaning of those endings.
    \end{itemize}
  \item The FOMA file that specifies more complicated replacement rules, for
    stem changes and other phenomena.
\end{itemize}

This initial system design is able to correctly analyze between 75 and 95\% of
the word forms of the open class nouns, adjectives and verbs, and all of the
closed class prepositions.  Continued development can easily proceed in a
modular fashion, either by improving the lexicon for an individual part of
speech (both in the script that determines continuation classes for each lemma,
and in the rule lexicon for that part of speech), or by improving the stem
changing rules in the FOMA file.

\section{System Analysis on Sloleks}

As of March 21, my morphological analyzer gets 100 percent recall for all word
forms in Sloleks (except those marked as spelling errors, which I did not test,
and a handful of words like ZPI192 that are misanalyzed).  That means that for
all of the word forms in Sloleks, my morphological analyzer predicts a set of
analyses that is a superset of the set of analyses provided by Sloleks.  The
analyzer also has 98.8 percent precision, meaning that for 98.8 percent of the
forms in Sloleks, every analysis predicted for that form is listed in Sloleks
as a correct analysis.  The remaining precision errors are almost entirely for
gender-restricted proper nouns and adjectives that are difficult to predict
(e.g., my system will produce feminine forms for a few roots that should only
have masculine forms, and which for some reason were not found as such by my
lexicon generation scripts).  I obtained 100 percent recall by automatically
adding special cases in the lexicon for every form that was incorrectly
analyzed, after I was at approximately 96.5 percent correct.  Note that I could
have just produced the special case table from the beginning, and gotten 100
percent recall and precision by just having my morphological analyzer be a big
lookup table into the Sloleks lexicon.  Building the morphological analyzer as
I did gives a smaller, more intuitive and more generalizable morphological
analyzer, where adding new forms is relatively easy, if they fit the standard
morphology of the language.  Also, it will allow for relatively easy
development of an unknown word guesser, which is the subject of the next
section.

\section{Lessons Learned and Revised Design}

Given that my morphological analyzer has essentially a perfect analysis of all
of the forms in my lexicon, my next goal is to quantify how many and what kind
of unseen forms one can expect when using the analyzer with real data. I can
then properly prioritize the building of guessing functionality into the
analyzer.  To start on this, I added some scripts to scrape a popular Slovene
news site, rtvslo.si, and detect unseen words.  When scraping the top 5 stories
at any given time, I find that between 2 and 5 percent of the word tokens in
the news articles are word types that do not occur in Sloleks.  These consist
almost entirely of numbers (sequences of digits, not spelled-out numbers) and
proper names, with a few loan words, odd hyphenations, and abbreviations.
Thus, when building my guesser, I will first focus on numbers and proper names.

\section{Final Revisions}

As described in the previous section, I wrote a guesser for digits and for
proper names.  The digits guesser is perfect, as it is very easy to label a
sequence of digits as a numeral.  The proper name guesser produces the correct
analysis for all of the proper names I examined, but it also drastically
over-generates analyses.  The problem is that to account for all possible
proper names, including foreign names, to guesser has to be very general.  I
also wrote a simple guesser for a particular subtype of feminine nouns, and
that guesser works very well---perhaps if I can find a way to group proper
nouns into smaller categories that can be guessed, I can avoid over-generating
so much.  But I'm not optimistic about the prospect of a really tight proper
name guesser.

In the end, the augmented system was unable to parse fewer than 1\% of the
tokens in the news articles I tested on.  A large percentage of those
unparseable words were due to processing errors in my scraping script, which,
for instance, removed periods from abbreviations, turning the recognizable
``oz.'' into the unrecognizable ``oz'', or mishandled hyphens (hyphens separate
stems from endings in some words in Slovene, particularly letters or
abbreviations like ``a-ja'', and so should not be split, but in other words,
such as ``22-letna'', a split would be appropriate).  There are still
occasionally a few new words, like ``popkoncert'' and its inflection
``popkoncertu'', and misspellings, that are not handled by my analyzer.  But
the vast majority of words found in contemporary sources of Slovene language
are recognized and correctly parsed.

\section{Future Work}

The main piece left to build in this analyzer is a robust unseen word guesser.
As the previous two sections have shown, such a guesser is not really needed
except in very rare cases, but it would make the analyzer more complete and
better able to handle new words.  I laid some ground work for implementing a
guesser, and, as mentioned above, wrote a guesser for a set of feminine nouns.
That feminine noun guesser took a total of four lines of additional code in two
files, so I'm optimistic that a reasonable guesser for a large percentage of
open class words could be built with relatively little effort.  But that is
left to future work.

It is also left to future work to improve the test system to accurately test
how well the unknown guesser is doing---it could check, for example, how
frequently the guessed analyses match up with the analyses based on Sloleks.
And it might be good to only give guesses when the Sloleks output gives
nothing; this could be done with a smart use of FOMA's priority union.

\bibliographystyle{plainnat}
\bibliography{bib}
\label{lastpage}
\end{document}
