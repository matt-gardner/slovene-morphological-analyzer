{"name":"Slovene-morphological-analyzer","tagline":"A finite state morphological analyzer for the Slovene language.","body":"slovene-morphological-analyzer\r\n==============================\r\n\r\nA finite state morphological analyzer for the Slovene language, written by\r\n[Matt Gardner](http://www.cs.cmu.edu/~mg1/).\r\n\r\nThis analyzer is a fast, lightweight and easily extensible morphological\r\ndisambiguation tool written with [FOMA](https://code.google.com/p/foma/), an\r\nopen source reimplementation of Xerox's finite state tool, xfst.  The analyzer\r\ndisambiguates words into the [JOS](http://nl.ijs.si/jos/index-en.html)\r\nmorphosyntactic specifications\r\n([MSDs](http://nl.ijs.si/jos/msd/html-en/index.html)).  As an example, one\r\nmight input a word and get back a set of analyses:\r\n\r\n    $ foma\r\n    foma[0]: load slovene.bin\r\n    foma[1]: up\r\n    apply up> jezikoslovje\r\n    jezikoslovje+N+Common+Neut+Sing+Acc\r\n    jezikoslovje+N+Common+Neut+Sing+Nom\r\n\r\nor one could input an analysis and get an inflection:\r\n\r\n    foma[1]: down\r\n    apply down> jezikoslovje+N+Common+Neut+Plural+Gen\r\n    jezikoslovij\r\n\r\nThis kind of analyzer is useful for a range of downstream natural language\r\nprocessing tasks, like part of speech tagging and parsing, as well as for an\r\naid to learners of the language.  Its main benefit over a lookup table is its\r\nextensibility; in order to add a word to a lookup table, all of its inflections\r\nmust be manually, individually added.  With a finite state analyzer, only the\r\nlemma needs to be added, and the morphology built into the analyzer will handle\r\nthe inflection.  Finite state analyzers can also give reasonable guesses for\r\nunseen words, where lookup tables would fail to give any output, though this\r\nanalyzer still has only very limited support for unseen words.\r\n\r\nThe analyzer was built and tested using the\r\n[Sloleks](http://www.slovenscina.eu/sloleks/opis) lexicon of Slovene language.\r\nThis code contains a script that processes Sloleks and outputs a set of FOMA\r\nlexicon files for each part of speech, including whatever information is\r\nnecessary for a word's inflection (e.g., whether or not a final \"e\" is\r\nfleeting, as in \"pomemben\" -> \"pomembna\" vs. \"rumen\" -> \"rumena\").  Each part\r\nof speech then has rules for generating the regular and irregular morphology of\r\nthe language.\r\n\r\nThere are also scripts to test the output of analyzer against the MSDs\r\ncontained in Sloleks.  There are approximately 930,000 unique forms in Sloleks\r\n(except for those marked as spelling errors, which we have so far ignored),\r\nwith an average of 3 possible MSD analyses for each form.  The test scripts can\r\ntest each part of speech individually, or all of them together, to see if the\r\nset of analyses for each word matches what is found in Sloleks.  The system\r\ncurrently has 100% recall on that test, and 98.8% precision.  That means that\r\nfor every word in Sloleks, this morphological analyzer produces a set of MSDs\r\nthat is a superset of analyses given in Sloleks, and for 98.8%, the sets match\r\nexactly.  The remaining precision errors are mostly due to overprediction of\r\nnumber or gender, where a proper noun only has a singular form, or a possessive\r\nadjective only has a feminine form, or other similar issues.\r\n\r\n100% recall was acheived by running a separate script that found remaining\r\nerrors and automatically generated lexicon override files for the forms that\r\nwere not correctly processed.  This was done after the rules were such that\r\nabout 96.5% of all forms were correctly processed - in lieu of encoding all of\r\nthe long tail of irregular morphology by hand, this was done automatically.\r\n\r\nTo get some idea of the coverage of this analyzer, a popular Slovene news site,\r\nrtvslo.si, was scraped for textual content.  In the text of the most popular\r\narticles, on average only between 2 and 5 percent of the tokens in the article\r\nare not contained in the 930,000 forms in Sloleks.  The vast majority of these\r\ntokens are sequences of digits (e.g., \"2009\") or proper names.  The analyzer\r\nwas thus improved to give guesses for unseen words---the number guesser works\r\nwell, though the proper name guesser currently vastly over-generates analyses.\r\nBut taking those two classes of tokens out of the unseen pool leaves less than\r\n1% of the tokens unrecognized by this analyzer, and most of those are due to\r\nprocessing errors (e.g., taking the period off of \"oz.\", which would otherwise\r\nbe recognized as an abbreviation, but is left as \"oz\", which is unrecognized,\r\nor hyphenated words like \"22-letna\").  There is a script in this repository\r\nthat will perform this analysis on the current top articles on rtvslo.si.\r\n\r\nThe resulting analyzer is easily extensible in a number of ways.  If a newer,\r\nimproved version of Sloleks is released, the scripts here will easily process\r\nit and update the generated lexicon files, assuming the format is the same.  If\r\na person wants to tackle some piece of the long tail of irregular morphology\r\nthat is currently left to automated overrides, it is easy to modify a few rules\r\nin the files pertaining to that part of speech and run tests to see how much\r\nthe analyzer is improved.  And in implementing the number and proper name\r\nguessers, some initial work was also given to guessing common feminine nouns,\r\nwhich would cover the rare cases when a non-proper unseen word is encountered\r\nin real text.  This is still very preliminary, but future work could expand the\r\nunseen word guessing capabilities of the analyzer and add tests that compare\r\nthe guesses against the analyses given in Sloleks.  Because of the design of\r\nthe system, this should not be a lot of work for regular words; an addition of\r\nfour total lines of code in two files produced a guesser for feminine nouns\r\nending in \"ost\" that performs quite well.\r\n\r\n# License\r\n\r\nThe FOMA and other code contained in this repository are available to the\r\npublic under the terms of the GNU General Public License version 3, distributed\r\nhere in the `LICENSE` file.  The [Sloleks\r\nlexicon](http://www.slovenscina.eu/sloleks/opis) is available under a Creative\r\nCommons license ([CC-BY](http://creativecommons.org/licenses/by/2.0/), which\r\nrequires attribution).\r\n\r\n# User Guide\r\n\r\n### Processing Sloleks\r\n\r\nFrom the `data/` directory, run `python create_lexica.py` and `python\r\ncreate_tests.py`.  These commands will generate a number of files in the\r\n`lexica/` and `tests/` directories.  These files are checked in to the\r\nrepository, so if you modify the Sloleks processing scripts, you can see the\r\ndifference it made by running `git diff`.\r\n\r\n### Modifying lexical rules\r\n\r\nFor detailed information on `.lexc` and `.foma` files, look at FOMA's [Getting\r\nStarted guide](https://code.google.com/p/foma/wiki/GettingStarted).  Here I\r\nwill just explain the layout of the system.  Each part of speech has\r\n(generally) 4 `.lexc` files in the `lexica/` directory.  Using verbs as an\r\nexample, these are:\r\n\r\n* `verbs.lexc`, a file generated automatically by the `create_lexica.py`\r\n  script,\r\n* `verbs_rules.lexc`, a file written by hand that contains the basic\r\n  morphological rules to inflect verbs,\r\n* `verbs_overrides.lexc`, a file written by hand that contains irregular\r\n  morphology for a small number of highly irregular words, and\r\n* `verbs_auto_overrides.lexc`, a file generated automatically by running the\r\n  `create_auto_overrides.py` script (described later).\r\n\r\nThere is also a single file containing regular expression rules that perform\r\nstem changes and other complex morphological processes.  This file is\r\n`foma/slovene.foma`.\r\n\r\n### The `scripts/` directory\r\n\r\nThe `scripts/` directory contains four useful scripts, each of which is\r\ndescribed below.  All of these scripts should be run from the base directory,\r\nwith `python scripts/[script].py`.\r\n\r\n* `make_bin.py`: This will combine all of the `.lexc` and `.foma` files, run\r\n  them through FOMA, and produce a file called `slovene.bin`, which is suitable\r\n  for use with `flookup`, or for loading into `foma`.  For more information on\r\n  the use of those tools, see FOMA's user guide (linked above).\r\n* `test.py`: This script first calls `make_bin.py`, then uses the resulting\r\n  binary and the tests in the `tests/` directory to judge the accuracy of the\r\n  analyzer.  For usage information run `python scripts/test.py --help`.  The\r\n  results are shown in the `results/` directory.  The main results file is\r\n  `results/[test]_stats.tsv`, which is suitable to be viewed with `sort`.\r\n* `find_errors.py`: This will look through the error file for a particular test\r\n  and select only those whose MSD matches some input.  This is useful, for\r\n  example, if you want to see errors on only a particular kind of pronoun, even\r\n  though you can only run tests on pronouns as a whole.\r\n* `create_auto_overrides.py`: This file looks at the error file for a\r\n  particular test, as well as the test file (originally generated from\r\n  Sloleks), and produces an `*_auto_overrides.lexc`.  If you are working to\r\n  improve the analyzer, you should run tests without the auto overrides (with\r\n  the `--no-auto-overrides` option), then re-run this script to reproduce the\r\n  (now hopefully smaller) `*_auto_overrides.lexc` file.\r\n\r\n### The `scraping/` directory\r\n\r\nIn this directory there is a script for scraping a few news articles from\r\nrtvslo.si and running some tests to see what unseen words there are.  This\r\nscript is currently written to be run from the `scraping/` directory, not the\r\nbase directory.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}